{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESRGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a simple implementation of the SRGAN super resolution architecture. Further Improvements need to be done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key takeaways from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ability of Mean Squared Error, Peak Signal to Noise Ratio to capture 'perceptually relevant'(like texture for instance) differences like high texture details is limited as both are based on pixel wise image differences. \n",
    "\n",
    "- Super resolved image may not be as photo realistic as the original image. \n",
    "\n",
    "- A Super Resolution Generative Adversarial Network is proposed, in which a deep residual network(ResNet) with skip connections. \n",
    "\n",
    "- A novel loss function is also proposed which uses high level feature maps of the VGG network. \n",
    "\n",
    "- Low resolution images were obtained by applying a Gaussian filter to the High Resolution Image, proceeded by a downsampling with a factor of r. \n",
    "\n",
    "- The ParametricReLU is used as the activation function. \n",
    "\n",
    "- The perceptual loss function was defined as a sum of content loss and adversarial loss. Weightage of the loss function is $1 : \\frac{1}{1000}$(content loss : adversarial loss)\n",
    "\n",
    "- Perceptual Loss > can use many humans to eval images, a far practical method is to use a pre trained network that has been trained on millions of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things to read up on:\n",
    "- MSE, PSNR \n",
    "- ResNet, skip connections\n",
    "- Loss functions\n",
    "- feature maps, vgg network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tI0KcjC3auG4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x2818f1f9d50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from os.path import join\n",
    "from torch import nn, optim\n",
    "from torchvision.models.vgg import vgg16\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from time import time\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from math import log10, sqrt \n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.ndimage import rotate\n",
    "from time import time\n",
    "import warnings as wr\n",
    "wr.filterwarnings(\"ignore\")\n",
    "from pylab import rcParams\n",
    "#rcParams['figure.figsize'] = 9,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y9gCiSoBdnXC"
   },
   "outputs": [],
   "source": [
    "UPSCALE_FACTOR = 4\n",
    "CROP_SIZE = 88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1JxvJbk7dqlY"
   },
   "outputs": [],
   "source": [
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(i, j): \n",
    "    mse = np.mean((i - j) ** 2) \n",
    "    if(mse == 0):  # MSE is zero means no noise is present in the signal . \n",
    "                  # Therefore PSNR have no importance. \n",
    "        return 100\n",
    "    max_pixel = 255.0\n",
    "    psnr = 20 * log10(max_pixel / sqrt(mse)) \n",
    "    return psnr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSIM(i,j):\n",
    "    i = cv2.cvtColor(i, cv2.COLOR_BGR2GRAY)\n",
    "    j = cv2.cvtColor(j, cv2.COLOR_BGR2GRAY)\n",
    "    s = ssim(i, j)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(i, j):\n",
    "    # the 'Mean Squared Error' between the two images is the sum of the squared difference between the two images\n",
    "    mse_error = np.sum((i.astype(\"float\") - j.astype(\"float\")) ** 2)\n",
    "    mse_error /= float(i.shape[0] * i.shape[1])\n",
    "    # return the MSE. The lower the error, the more \"similar\" the two images are.\n",
    "    return mse_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readRGBImage(path):\n",
    "    '''\n",
    "    A simple function to read images in RGB, as OpenCV reads images in BGR, and can mess up displaying images. \n",
    "    \n",
    "    inputs: \n",
    "        path(str): this is the path of the image\n",
    "    '''\n",
    "    return cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScaleImageByRatio(image, factor, interpolationMethod = cv2.INTER_LINEAR):\n",
    "    '''\n",
    "    Scale the image using a scaling factor\n",
    "    \n",
    "    inputs:\n",
    "        image - input image as a matrix\n",
    "        factor - scale factor for both length and width \n",
    "        interpolationMethod - interpolation method used\n",
    "    '''\n",
    "    \n",
    "    return cv2.resize(image,None,fx=factor, fy=factor, interpolation = interpolationMethod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DisplayImageComparison1x3(hr, lr, sr, lr_title = 'LR Image', sr_title = 'SR Image', metrics=False):\n",
    "    '''\n",
    "    This function shows a comparason of the ground truth HR images, LR images and the SR images. \n",
    "    \n",
    "    input: \n",
    "        hr(np.ndarray): the HR, ground-truth Image\n",
    "        lr(np.ndarray): the LR Image\n",
    "        hr(np.ndarray): the SR Image.\n",
    "    '''\n",
    "    if metrics:\n",
    "        sr_title = f'SR Image\\nPSNR: {PSNR(sr,hr):.3f}, SSIM: {SSIM(sr,hr):.3f}'\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(11,5))\n",
    "    ax = axes.ravel()\n",
    "    ax[0].imshow(hr)\n",
    "    ax[0].set_title('HR Image')\n",
    "    ax[1].imshow(lr)\n",
    "    ax[1].set_title(lr_title)\n",
    "    ax[2].imshow(sr)\n",
    "    ax[2].set_title(sr_title)\n",
    "    #plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGPU_details(device):\n",
    "    '''\n",
    "    A simple function which prints information about your gpu. \n",
    "    '''\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Total Memory: {round(torch.cuda.get_device_properties(0).total_memory/1024**3,1)} GB')\n",
    "    print('Memory Usage:')\n",
    "    print(f'Allocated: {round(torch.cuda.memory_allocated(0)/1024**3,1)} GB')\n",
    "    print(f'Cached: {round(torch.cuda.memory_reserved(0)/1024**3,1)} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional Methods\n",
    "![](upsampling_methods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbour Interpolation\n",
    "\n",
    "Nearest Neighbour Interpolation in Images is a simple method used to resize images by determining the intensity value of new pixels based on the nearest existing pixel. Imagine you have a small picture and you want to make it bigger. Nearest Neighbor Interpolation helps decide what color each new pixel should be by looking at the closest original pixels. It's like if you have a small drawing and you want to make it twice as big, you would look at each small square and color the new big squares based on the closest small square's color. This method is quick and easy to understand, making it a common way to resize images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = readRGBImage('source/test_images/baboon.png')\n",
    "j = readRGBImage('source/test_images/baboon_test.png')\n",
    "a = ScaleImageByRatio(j, UPSCALE_FACTOR, interpolationMethod=cv2.INTER_NEAREST)\n",
    "DisplayImageComparison1x3(i, j, a, sr_title='Image Upscaled by\\nNearest Neighbour Interpolation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilinear Interpolation\n",
    "\n",
    "First, we perform linear interpolation in the x-direction at the y-coordinate of the new pixel. This means we find the values of the two closest pixels along the same row as the new pixel and calculate a weighted average of those two values based on the distance of the new pixel from each of those pixels.\n",
    "\n",
    "Next, we perform linear interpolation in the y-direction at the x-coordinate of the new pixel. This means we find the values of the two closest pixels along the same column as the new pixel and calculate a weighted average of those two values based on the distance of the new pixel from each of those pixels.\n",
    "\n",
    "Finally, we take the weighted average of the two interpolated values obtained in the previous steps to get the final value of the new pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = readRGBImage('source/test_images/baboon.png')\n",
    "j = readRGBImage('source/test_images/baboon_test.png')\n",
    "a = ScaleImageByRatio(j, UPSCALE_FACTOR, interpolationMethod=cv2.INTER_LINEAR)\n",
    "DisplayImageComparison1x3(i, j, a, sr_title='Image Upscaled by\\nBilinear Interpolation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bicubic Interpolation\n",
    "\n",
    "Bilinear Interpolation uses the 4 closest pixels to estimate the value of a new pixel, whereas Bicubic Interpolation uses the 16 closest pixels. This allows Bicubic Interpolation to produce smoother and more accurate results, especially for images with fine details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = readRGBImage('source/test_images/baboon.png')\n",
    "j = readRGBImage('source/test_images/baboon_test.png')\n",
    "a = ScaleImageByRatio(j, UPSCALE_FACTOR, interpolationMethod=cv2.INTER_CUBIC)\n",
    "DisplayImageComparison1x3(i, j, a, sr_title='Image Upscaled by\\nBicubic Interpolation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains code for the dataloader needed for the data in the folder. This is a custom implementation of the `TrainDatasetFromFolder` class, which inherits from the original class in the `pytorch` library. The dataloader returns a HR(High Resolution, which is the image at its original resolution) image and an LR image(which has been resized to introduce image degradation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we used in this notebook is the [Alsat-2B satellite dataset](https://github.com/achrafdjerida/Alsat-2B), which was introduced in the paper, [A New Public Alsat-2b dataset for Single-Image Super-Resolution](https://arxiv.org/pdf/2103.12547.pdf) which contains 2182 training image pairs of HR and LR images. The HR images have dimensions of 256x256 pixels, whereas the LR images have dimensions of 64x64 pixels. The Test dataset is split into 3 categories, each containing scenes from Agriculture, Urban, and a special category. These categories have 56 HR-LR image pairs each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
    "    return crop_size - (crop_size % upscale_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hr_transform(crop_size):\n",
    "    return Compose([\n",
    "        RandomCrop(crop_size),\n",
    "        ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr_transform(crop_size, upscale_factor):\n",
    "    return Compose([\n",
    "        ToPILImage(),\n",
    "        Resize(crop_size // upscale_factor, interpolation=Image.BICUBIC),\n",
    "        ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_transform():\n",
    "    return Compose([\n",
    "        ToPILImage(),\n",
    "        Resize(400),\n",
    "        CenterCrop(400),\n",
    "        ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "N0tpRB2Jdys4"
   },
   "outputs": [],
   "source": [
    "class TrainDatasetFromFolder(Dataset):\n",
    "    def __init__(self, dataset_dir, crop_size, upscale_factor):\n",
    "        super(TrainDatasetFromFolder, self).__init__()\n",
    "        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n",
    "        crop_size = calculate_valid_crop_size(crop_size, upscale_factor)\n",
    "        self.hr_transform = train_hr_transform(crop_size)\n",
    "        self.lr_transform = train_lr_transform(crop_size, upscale_factor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        hr_image = self.hr_transform(Image.open(self.image_filenames[index]))\n",
    "        lr_image = self.lr_transform(hr_image)\n",
    "        return lr_image, hr_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "re2-OAGQv-aQ",
    "outputId": "c728e7af-8eac-4f92-b125-5342ccf6586d"
   },
   "outputs": [],
   "source": [
    "train_set = TrainDatasetFromFolder(\"source/Alsat-2B/Train/HR\", crop_size=CROP_SIZE,\n",
    "                                   upscale_factor=UPSCALE_FACTOR)\n",
    "trainloader = DataLoader(train_set, batch_size=64, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cA8VHoPTwMaz"
   },
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SRGAN Model Architecture](images/esrgan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator architecture\n",
    "\n",
    "The Discriminator block consists of a block which contains a convolutional layer, followed by batch normalisation, followed by an activation layer. The discriminator block then progressess to have the number of filters multiplied by 2(first block has 64 filters, next block has 128 filters, next block has 256 and so on and so forth). In addition, blocks with the same number of filters have alternating stride(2 blocks having 128 filters have strides of 1 for the first block, and 2 for the second block, same can be observed with the 256 filters blocks and the 512 filters blocks). This is then fed into a dense layer with 1024 nodes, which is fed into an activation function(LeakyReLU) and then into another dense layer with 1 node, whose output is fed into a sigmoid activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first major block consists of a sequence of residual blocks, after the input layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Residual Block?\n",
    "\n",
    "A Residual block consists of the following components:\n",
    "- A series of convolutional layers that are responsible for extracting features from the input data. \n",
    "- Batch normalisation is applied after each convolutional layer. This helps in stabilising and accelerating training by normalising the input to the following layer. \n",
    "- Activation functions are applied to introduce non linearity in the network, which allows the network to learn complex and non linear patterns in the data. \n",
    "- Skip Connections which are essentially the sum of the input of the block and the output of the last batch normalisation layer, allows gradients to flow more freely throught the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A residual block is a fundamental component of ResNet, a model for extracting features from an image. It is designed to address the degradation problem in deep neural networks, which occurs when the performance of a deep neural network decreases as the number of layers increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Read up on the Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNpcnPmbw6QQ"
   },
   "outputs": [],
   "source": [
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, up_scale):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2,\n",
    "                              kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n",
    "        self.prelu = nn.PReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        x = self.prelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Residual Dense Blocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DQGAwDSGwUnE"
   },
   "outputs": [],
   "source": [
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, inChannels=64, outChannels=32, beta=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(inChannels, outChannels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(inChannels+outChannels, outChannels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(inChannels + 2 * outChannels, outChannels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(inChannels + 3 * outChannels, outChannels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(inChannels + 4 * outChannels, outChannels, kernel_size=3, stride=1, padding=1)\n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.beta = beta\n",
    "    \n",
    "    def forward(self, x):\n",
    "        block1 = self.act(self.conv1(x))\n",
    "        block2 = self.act(self.conv2(torch.cat((block1, x), dim = 1)))\n",
    "        block3 = self.act(self.conv3(torch.cat((block2, block1, x), dim = 1)))\n",
    "        block4 = self.act(self.conv4(torch.cat((block3, block2, block1, x), dim = 1)))\n",
    "        out = self.conv5(torch.cat((block4, block3, block2, block1, x), dim = 1))\n",
    "        return x + self.beta * out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Residual in Residual Dense Blocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualInResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, inChannels=64, outChannels=32, beta=0.2):\n",
    "        super().__init__()\n",
    "        self.RDB = ResidualDenseBlock(inChannels, outChannels)\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.RDB(x)\n",
    "        out = self.RDB(out)\n",
    "        out = self.RDB(out)\n",
    "        return x + self.beta * out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No of Residual-in-Residual Blocks: 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZZnfl5c0uzUI"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, inChannels=3, outChannels=3, n_blocks=23):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        RRDB = ResidualInResidualDenseBlock()\n",
    "        RRDB_layer = []\n",
    "        for i in range(n_blocks):\n",
    "            RRDB_layer.append(RRDB)\n",
    "        self.RRDB_block = nn.Sequential(*RRDB_layer) # *RRDB_Layer is a packing argument. \n",
    "        self.RRDB_conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.upconv = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.output_conv = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        first_conv = self.conv1(x)\n",
    "        RRDB_block = torch.add(self.RRDB_conv2(self.RRDB_block(fist_conv)), first_conv)\n",
    "        upsample_1 = self.upconv(f.interpolate(RRDB_block, scale_factor=2))\n",
    "        upsample_2 = self.upconv(f.interpolate(upsample_1, scale_factor=2))\n",
    "        output = self.output_conv(upsample_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ue5xbeyUuzWF"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 1024, kernel_size=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(1024, 1, kernel_size=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        batch_size=x.size()[0]\n",
    "        return torch.sigmoid(self.net(x).view(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of loss functions have been devised over the past years to train SR models. These can be categorised into 2 categories: \n",
    "\n",
    "1. Pixel Based Losses\n",
    "2. Network Based Lossed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel Based Losses\n",
    "\n",
    "- Pixel Loss(MSE): This is Mean Squared Error, in which each pixel of the SR image is compared against the corresponding pixel in the HR image. \n",
    "- Mean Absolute Error: This is the sum of the absolute difference between the predicted values of the model and the actual target values. \n",
    "- Total Variation Loss(TV): This method is used to supress the noise in the generated image. It measures the noise in the image by taking the absolute difference between neighbouring pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Based Losses\n",
    "\n",
    "- Perceptual Losses: This loss function evaluates based on the perceptual quality of the generated SR image as compared to the perceptual quality of the HR image. The way this is implemented is using the high level features of a pretrained image classification network. \n",
    "- Texture Loss: This loss function is used to enable the generated SR image to have the same style as the HR image(can include texture, colour, contrast in the image). \n",
    "- Adversarial Loss: In essence this is the min-max loss function. The generator tries to maximise, while the discriminator tries to minimise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SRGAN Loss function \n",
    "\n",
    "The paper proposed a perceptual loss function, which consists of an adversarial loss and a content loss. The content loss is useful for comparing the features of the reconstructed images with the original images, focusing on differences in features rather than differences in pixel levels in attributes like colour and brightness. <br>\n",
    "\n",
    "In addition, the SRGAN model uses the VGG19 network to extract image features for content loss calculation. The VGG19 convolutional neural network was developed by Karen Simoyan and Andrew Zisserman at the University of Oxford in 2014. The network contains 19 trainable networks, out of which, 16 of those layers are convolutional. The network relies on 3x3 convolutions with a stride of 1(Stride refers to the number of pixels by which we move the filter/kernel across the input image).<br>\n",
    "\n",
    "By utilising  the VGG19 network, the SRGAN model can extract complex features from images and compare them to enhance image quality. Additionally, the VGG loss helps in preserving high frequency information and fine details in the reconstructed images. The adversarial loss function in the SRGAN model also encourages the generator to produce images that closely resemble high resolution images, further enhancing the overall performance of the model. <br>\n",
    "\n",
    "As stated earlier, the perceptual loss function consists of an adverserial loss and a content loss. The equation for them is given below. \n",
    "\n",
    "$$ \\large l^{\\text{SR}} = \\overbrace{\\underbrace{l_x^{\\text{SR}}}_{\\text{Content Loss}} + \\underbrace{10^{-3}l_{\\text{Gen}}^{\\text{SR}}}_{\\text{Adversarial Loss}}}^{\\text{Perceptual Loss}} $$\n",
    "\n",
    "As we can see from the equation, the content loss plays a major role in the loss function of the SRGAN loss function, while the adversarial loss function only has $\\frac{1}{1000}$th of weightage in the loss function. <br>\n",
    "\n",
    "The content loss is composed of 2 parts, MSE Loss, and VGGLoss. The MSE Loss is the pixel wise error between the HR image and the SR image, whereas the VGG loss indicates the feature map obtained by the j-th convolution(post activation) before the i-th maxpooling layer within the VGG19 network. This is also defined as the euclidean distance between the feature representations of a reconstructed image and the reference image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TVLoss` Class defined below calculates Total Variable Loss, which measures the smoothness of a tensor, by encouranging small differences between neighbouring pixels. Horizontal and vertical variations are also considered, and a single loss value is returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RamJwy7yxitq"
   },
   "outputs": [],
   "source": [
    "class TVLoss(nn.Module):\n",
    "    def __init__(self, tv_loss_weight=1):\n",
    "        super(TVLoss, self).__init__()\n",
    "        self.tv_loss_weight=tv_loss_weight\n",
    "  \n",
    "    def forward(self, x):\n",
    "        batch_size=x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "\n",
    "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
    "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
    "\n",
    "        h_tv = torch.pow(x[:, :, 1:, :] - x[:, :, :h_x - 1, :], 2).sum()\n",
    "        w_tv = torch.pow(x[:, :, :, 1:] - x[:, :, :, :w_x - 1], 2).sum()\n",
    "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
    "\n",
    "    @staticmethod # Must add this\n",
    "    def tensor_size(t):\n",
    "        return t.size()[1] * t.size()[2] * t.size()[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GeneratorLoss` Class is where we have initialised the loss function of the model. This contains of various parts: \n",
    "- `vgg`: The pretrained VGG-16 network, which is a CNN used for image recognition. The `pretrained=True` arguments means that the weights of the network are initialised with the weights trained on the ImageNet dataset. \n",
    "- `loss_network`: This is a subnetwork of the VGG network, containing the first 31 layers of the network. This subnet is used to calculate the perception loss. \n",
    "- `mse_loss`: This is the MSE loss function, used to compare generated images with target images. \n",
    "- `tv_loss`: This is the total variation loss used to smooth out the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiwFaF3tySU_"
   },
   "outputs": [],
   "source": [
    "class GeneratorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneratorLoss, self).__init__()\n",
    "        vgg = vgg16(pretrained=True)\n",
    "        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n",
    "        for param in loss_network.parameters():\n",
    "          param.requires_grad = False\n",
    "        self.loss_network = loss_network\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.tv_loss = TVLoss()\n",
    "        \n",
    "    def forward(self, out_labels, out_images, target_images):\n",
    "        adversial_loss = torch.mean(1 - out_labels)\n",
    "        perception_loss = self.mse_loss(out_images, target_images)\n",
    "        image_loss = self.mse_loss(out_images, target_images)\n",
    "        tv_loss = self.tv_loss(out_images)\n",
    "        return image_loss + 0.001 * adversial_loss + 0.006 * perception_loss + 2e-8 * tv_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Selection\n",
    "\n",
    "This next code cell initialises the device that will be used for training. Since this is trained on a local machine, the model will choose a CUDA capable device(in this case, a Mobile NVIDIA RTX 3070Ti GPU with 8GB of VRAM). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tzzSCJjjy7I9",
    "outputId": "50699703-68e5-4767-d034-d029bfdb66be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "Total Memory: 8.0 GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    getGPU_details(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oGBnX7PLGJ-"
   },
   "outputs": [],
   "source": [
    "netG = Generator(UPSCALE_FACTOR)\n",
    "netD = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n39cIDQeLJFO",
    "outputId": "1ed2620f-7ef2-4329-8335-152d9b5e8b15"
   },
   "outputs": [],
   "source": [
    "generator_criterion = GeneratorLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ynXcO-8LSgN"
   },
   "outputs": [],
   "source": [
    "generator_criterion = generator_criterion.to(device)\n",
    "netG = netG.to(device)\n",
    "netD = netD.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDlYj1loLiD_"
   },
   "outputs": [],
   "source": [
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002)\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doBMeLWOLs_0"
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"d_loss\":[],\n",
    "    \"g_loss\":[],\n",
    "    \"d_score\": [],\n",
    "    \"g_score\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeTxlp0eML1d"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCheckpoint(model, optimizer, filename):\n",
    "    checkpoint = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint,filename)\n",
    "    print(f'Saved checkpoint {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCheckpoint(checkpoint_file, model, optimizer, lr): \n",
    "    print('===> load checkpoint')\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    for param_group in optimizer.param_groups:      # needed or model will have learning rate of older checkpoint \n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Code block below implements the training of the SRGAN model. The discriminator network is updated first, followed by the generator network. The process is repeated for each batch of data until the end of the epoch is reached. The running totals of the losses and scores are used to compute the average loss and score for each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHadZtxcMO01",
    "outputId": "45a6effa-74c9-40a7-84d7-1d0bda73e926",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    train_bar = tqdm(trainloader, file=sys.stdout)\n",
    "    running_results = {'batch_sizes':0, 'd_loss':0,\n",
    "                     \"g_loss\":0, \"d_score\":0, \"g_score\":0}\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    for data, target in train_bar:\n",
    "        g_update_first = True\n",
    "        batch_size = data.size(0)\n",
    "        running_results['batch_sizes'] += batch_size\n",
    "\n",
    "        real_img = Variable(target)\n",
    "        real_img = real_img.to(device)\n",
    "        z = Variable(data)\n",
    "        z = z.to(device)\n",
    "\n",
    "        ## Update Discriminator ##\n",
    "        fake_img = netG(z)\n",
    "        netD.zero_grad()\n",
    "        real_out = netD(real_img).mean()\n",
    "        fake_out = netD(fake_img).mean()\n",
    "        d_loss = 1 - real_out + fake_out\n",
    "        d_loss.backward(retain_graph = True)\n",
    "        optimizerD.step()\n",
    "\n",
    "        ## Now update Generator\n",
    "        fake_img = netG(z)\n",
    "        fake_out = netD(fake_img).mean()\n",
    "        netG.zero_grad()\n",
    "        g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
    "        g_loss.backward()\n",
    "\n",
    "        fake_img = netG(z)\n",
    "        fake_out = netD(fake_img).mean()\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        running_results['g_loss'] += g_loss.item() * batch_size\n",
    "        running_results['d_loss'] += d_loss.item() * batch_size\n",
    "        running_results['d_score'] += real_out.item() * batch_size\n",
    "        running_results['g_score'] += real_out.item() * batch_size\n",
    "\n",
    "        ## Updating the progress bar\n",
    "        train_bar.set_description(desc=\"[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f\" % (\n",
    "            epoch, N_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
    "            running_results['g_loss'] / running_results['batch_sizes'],\n",
    "            running_results['d_score'] / running_results['batch_sizes'],\n",
    "            running_results['g_score'] / running_results['batch_sizes']\n",
    "        ))\n",
    "    results['d_loss'].append(running_results['d_loss'])\n",
    "    results['g_loss'].append(running_results['g_loss'])\n",
    "    results['d_score'].append(running_results['d_score'])\n",
    "    results['g_score'].append(running_results['g_score'])\n",
    "    \n",
    "    if epoch%50==0:\n",
    "        saveCheckpoint(netG, optimizerG, filename='checkpoints/generator_checkpoint.pth')\n",
    "        print(f'Saved Generator Checkpoint')\n",
    "        saveCheckpoint(netD, optimizerD, filename='checkpoints/discriminator_checkpoint.pth')\n",
    "        print(f'Saved Discriminator Checkpoint')\n",
    "    netG.eval()\n",
    "print(f'Total Training Time: {(time() - t0)/3600} Hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0u95d5GJkfAf"
   },
   "source": [
    "# An attempt at testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(input_file, save_image=False):\n",
    "    '''\n",
    "    This function passes an image into the model, and retrieves its input. \n",
    "    Need to implement model saving and checkpointing. \n",
    "    '''\n",
    "    # Read input image and convert to Tensor for feeding into model\n",
    "    image = cv2.imread(input_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Need to add this as OpenCV uses BGR instead of RGB\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    in_tensor = transform(image)\n",
    "    \n",
    "    # Feed input tensor into model\n",
    "    in_tensor = in_tensor.to(device) # Send input to device, can cause problems if input tensor is on CPU and Model on GPU!!\n",
    "    out_tensor = netG(in_tensor.unsqueeze(0))\n",
    "    \n",
    "    # Final transformations, as for some reason, output is rotated\n",
    "    display_tensor = out_tensor.to('cpu') # send output tensor back to CPU\n",
    "    display_tensor = display_tensor.detach().numpy()\n",
    "    display_tensor = np.transpose(display_tensor)\n",
    "    display_tensor = np.squeeze(display_tensor)\n",
    "    display_tensor = cv2.flip(display_tensor, 0)\n",
    "    display_tensor = rotate(display_tensor, -90)\n",
    "    f = (display_tensor * 255).astype(np.uint8)\n",
    "    if save_image:  \n",
    "        out_file = input('What name do you want to save the image as: ')\n",
    "        cv2.imwrite(f'{out_file}.png', f)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = readRGBImage('Alsat-2B/Test/Agriculture/HR/HR_12.jpg')\n",
    "j = readRGBImage('Alsat-2B/Test/Agriculture/LR/LR_12.jpg')\n",
    "a = testModel('Alsat-2B/Test/Agriculture/LR/LR_12.jpg')\n",
    "DisplayImageComparison1x3(i, j, a, metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = readRGBImage('Alsat-2B/Test/Agriculture/HR/HR_51.jpg')\n",
    "j = readRGBImage('Alsat-2B/Test/Agriculture/LR/LR_51.jpg')\n",
    "a = testModel('Alsat-2B/Test/Agriculture/LR/LR_51.jpg')\n",
    "DisplayImageComparison1x3(i, j, a, metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = readRGBImage('Alsat-2B/Test/Agriculture/HR/HR_11.jpg')\n",
    "j = readRGBImage('Alsat-2B/Test/Agriculture/LR/LR_11.jpg')\n",
    "a = testModel('Alsat-2B/Test/Agriculture/LR/LR_11.jpg')\n",
    "DisplayImageComparison1x3(i, j, a, metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot([i/100 for i in results['d_loss']], label='Discriminator Loss')\n",
    "plt.plot([i/100 for i in results['g_loss']], label='Generator Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Discriminator vs Generator Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References: \n",
    "\n",
    "1.  Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A., ... & Shi, W. (2017). Photo-realistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4681-4690).\n",
    "2. Djerida, A., Djerriri, K., & Karoui, M. S. (2021, July). A new public Alsat-2B dataset for single-image super-resolution. In 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS (pp. 8095-8098). IEEE.\n",
    "3. Shi, W., Caballero, J., Huszár, F., Totz, J., Aitken, A. P., Bishop, R., ... & Wang, Z. (2016). Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1874-1883)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "superres",
   "language": "python",
   "name": "superres"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
